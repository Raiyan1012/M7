# -*- coding: utf-8 -*-
"""Homechat_Project_ENG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/136hdhxLNCblBjI6dSzeWSkVrULuS90CA

# Project: **HomeChat:**  *Enhancing Real Estate Decision-Making with AI*

## **Business understanding**

As master's students in Business Data Science, we are thrilled to undertake a project in collaboration with HomeChat AI, an innovative conversational AI platform specializing in real estate insights for private persons wishing to acquire property. This project will leverage our knowledge of data science, and in this case, offering an unique opportunity to apply it to a real-world business model.


**Objective: Bridging the assymentry information Gap**

---



One prevalent challenge in the real estate market is information asymmetry. Potential buyers, particularly first-time entrants, may lack comprehensive data on property details, financial options, and the overall buying process. This gap can lead to less informed decisions and potential missteps.

In light of this, our project seeks to enrich the HomeChat platform with mortgage loan information from various Danish bank websites. We aim to augment HomeChat's existing services and create a more holistic platform, transforming it into a one-stop solution for all real estate queries, thereby bridging the information gap.

**Business Model: A Value-Driven Approach**


---

The business model relies publicly accessible information. Instead of incurring high costs associated with licensing or purchasing proprietary market research data, we are leveraging freely available data from bank, and fianance websites. This innovative approach ensures cost-effectiveness while enabling us to gather pertinent and current financial data.

The integration of this information is set to greatly enhance the value proposition of HomeChat AI. By extending its capabilities to provide financial advice on mortgage loans, HomeChat will become an even more comprehensive solution for real estate buyers. Users will have access to a wealth of property-related information as well as insights on financing options, all from a single platform.

With this project, we aim to address the information asymmetry in the real estate market head-on. The successful application of our Business Data Science expertise will make the journey of buying real estate more transparent and less daunting, particularly for first-time buyers. Ultimately, our mission is to leverage the power of data science and AI to revolutionize real estate decision-making, making it more accessible and empowering for all.

## **Data Collection**

The data collection process is designed to extract relevant information about the mortgage loan processes and the types of loans available from the targeted Danish bank websites. Our Python script will be used for this purpose, boasting the ability to scrape both HTML and PDF content.

The bank websites offer a wealth of publicly accessible information that can substantially augment the value HomeChat offers to its users. By retrieving data about loan types, interest rates, and loan application requirements, HomeChat will be positioned to provide users with personalized financial advice that aligns with their property interests.

Post-scraping, the data will undergo rigorous cleaning and preprocessing to ensure it is in a suitable format for further processing. This includes the removal of duplicates, irrelevant information, and addressing other inconsistencies to yield a clean and structured dataset.

Following the data collection and cleaning phases, the data will be tokenized and embedded, to be stored in a Pinecone database, a dedicated vector database. Ultimately, a language chain agent will leverage this stored data, marking the final step in our methodology and enabling HomeChat to provide detailed and accurate financial insights to its users.

## Document embedding (Through OpenAI)

The document embedding stage leverages the power of OpenAI's text embeddings API. Text embeddings are numerical representations of texts that can be used to measure the semantic relatedness of text strings. They have found extensive applications in tasks like search, clustering, recommendations, anomaly detection, diversity measurement, and classification.

**Embedding Generation**

We use OpenAIâ€™s API to generate embeddings. The process involves sending our preprocessed text strings to the embeddings API endpoint along with a choice of embedding model ID, for instance, "text-embedding-ada-002". The response from the API includes an embedding for each text string, which we can then extract, store, and use.

An embedding is essentially a vector or list of floating-point numbers. The distance between two vectors is indicative of their relatedness - small distances suggest high relatedness and large distances suggest low relatedness.

Here's an example of how to get embeddings:
"""

#response = openai.Embedding.create(
    #input="Your text string goes here",
    #model="text-embedding-ada-002"
#)
#embeddings = response['data'][0]['embedding']

#In our case, "Your text string goes here" will be replaced by the data extracted from the bank websites.

"""The embeddings generated by the OpenAI API are based on the second-generation model. In comparison to the former embedding models by OpenAI, this model provides a overall better performance because it is better, cheaper, and simpler to use. 

**Application of Document Embedding**

Once the embeddings for all the documents are generated, we store them in a vector database. This database serves as the basis for our language model agent to leverage the data.

In the context of HomeChat, our conversational AI, the document embeddings can be instrumental in understanding user queries related to real estate listings, extracting insights from floor plans, images, and more. For instance, they can help the AI understand and provide accurate responses to complex user queries about mortgage loan options and their specifics available on different bank websites.

Through this process, we aim to empower HomeChat with the capability to provide highly accurate, context-aware, and valuable responses to user queries, thereby enhancing the user experience and the overall value proposition of our platform.

## Project Significance

The real estate sector is a complex field, characterized by an abundance of information that is often disparate, hard to comprehend, and sometimes inaccessible to potential property buyers. This is especially challenging for first-time buyers, who may find themselves overwhelmed and under-informed when making crucial decisions about property investments. This gap of information asymmetry can lead to less-than-optimal decision-making, which in turn can have significant financial implications.

Our project, in collaboration with HomeChat AI, aims to address this issue head-on. HomeChat AI is a pioneering startup, creating a conversational AI platform designed to offer comprehensive real estate insights to individuals looking to acquire property. By incorporating our understanding of data science, we aim to enhance the platform's capabilities and transform it into a comprehensive solution for all real estate related inquiries.

The goal is to bridge the information gap in the real estate market, particularly in the area of mortgage loans. By incorporating financial data sourced from various Danish bank websites into the HomeChat platform, we are creating a unique blend of property and financial data that prospective buyers can easily access and understand.

A major significance of our project lies in the business model's cost-effectiveness. Instead of resorting to expensive proprietary market research data, we leverage publicly accessible financial data, ensuring we provide current and pertinent information to users without incurring significant costs.

This integration of financial data into HomeChat AI enhances the platform's value proposition. The incorporation of mortgage loan information enables the platform to provide financial advice in addition to its existing property-related services. The result is a comprehensive platform where users can find property-related data and receive financial advice in one place.

By bringing financial insights into the fold, the platform will cater to users' needs more holistically, making the process of buying real estate less daunting, more transparent, and ultimately empowering for users. The combination of AI and data science in this way could be a game-changer in the real estate sector, making it easier for people to make informed decisions when buying properties.

To sum up, the project holds significant potential in addressing a pressing issue in the real estate market - information asymmetry. By integrating extensive financial information into HomeChat AI's real estate services, we aim to make the property buying process more accessible, transparent, and less intimidating for users, particularly first-time buyers. This project's successful execution will contribute significantly to enhancing the user experience and the overall real estate decision-making process.

## Methodology

The methodology for our project involves multiple stages, each contributing to the enhancement of HomeChat AI's service capabilities.

### **Data Collection:**

The initial phase involves the data collection from target Danish bank websites. We'll employ a Python script, tailored for scraping both HTML and PDF content, to extract relevant information about mortgage loan procedures and types of loans available. This process ensures we gather a broad range of data that can bolster HomeChat's offerings to users.

### **Data Cleaning and Preprocessing:**

After scraping, the data is subjected to rigorous cleaning and preprocessing. This includes eliminating duplicates, pruning irrelevant information, and resolving other inconsistencies to produce a clean, structured dataset ready for subsequent stages.

### **Document Embeddings:**

This stage leverages the power of OpenAI's text embeddings API. Document embeddings are numerical representations of text that aid in measuring semantic relatedness. After the data is processed, each document is transformed into an embedding - a vector of floating-point numbers - and stored in a vector database (Pinecone, in this case). These embeddings are integral to understanding user queries and provide accurate, context-aware responses.

### **Language Chain Agent:**

We will design a Language Chain Agent capable of leveraging the vector store to answer finance-related questions. The agent will be trained to recognize and respond to specific prompts about loan applications and types. Its performance will be evaluated and fine-tuned as required for optimal operation.

In conclusion, the project methodology is a comprehensive process designed to enhance the HomeChat AI platform. By integrating rich financial data into its service offerings, we aim to equip the platform with the ability to provide accurate, detailed responses to user queries, ultimately delivering a more complete and valuable service to users. We expect this project to make a significant contribution to addressing the information asymmetry issue in the real estate market.

# Programming section

## Data scrapping

This Python script written below is designed to seamlessly extract text data from both HTML websites and PDF files. By leveraging the power of the PyPDF2 and BeautifulSoup4 libraries, it provides an efficient and user-friendly solution for data scraping. 

The script is composed of several functions, each with a specific purpose: fetching HTML content, extracting text from HTML, extracting text from PDFs, and orchestrating the data scraping process. Users can easily customize the script by adding their desired HTML URLs and PDF file paths to the sources list. 

Once executed, the script will process the provided sources, extract the text content, and output the results, allowing users to harness the data for their own purposes.



---

**Installation and Imports**

The script begins by installing PyPDF2, a Python library used for reading PDF files. It then imports necessary libraries, including requests for sending HTTP requests, BeautifulSoup from bs4 for parsing HTML and XML documents, PyPDF2 for handling PDF files, and other utilities like os, tempfile, and shutil.
"""

!pip install PyPDF2

import requests
from bs4 import BeautifulSoup
import PyPDF2
import os
import tempfile
import shutil
from PyPDF2 import PdfReader

"""**This code defines a function called `get_html_data` that takes a URL as input and returns the HTML content of that URL. Here is a summary explanation of the code:**

1. The function `get_html_data` is defined with a single parameter, `url`, representing the URL of the webpage whose HTML content we want to retrieve.

2. Inside the function, a try-except block is used to handle any potential exceptions that may occur during the execution of the code.

3. The `requests.get(url)` function is called to send an HTTP GET request to the specified URL and retrieve the response.

4. The code checks the status code of the response using `response.status_code`. If the status code is 200, it means the request was successful.

5. If the status code is 200, the function returns the HTML content of the response using `response.text`.

6. If the status code is not 200, an error message is printed indicating that the URL couldn't be accessed.

7. If an exception occurs during the execution of the code (e.g., network error or invalid URL), the except block catches the exception, prints an error message, and returns `None`. 

Overall, this code provides a simple way to retrieve the HTML content of a webpage by providing its URL. It uses the `requests` library to send an HTTP request and handles different scenarios such as successful requests, unsuccessful requests, and exceptions.
"""

def get_html_data(url):
    """Give me a URL and I'll convert that HTML"""
    try:
        response = requests.get(url)
        if response.status_code == 200:
            return response.text
        else:
            print(f"Error {response.status_code}: Couldn't access {url}. Bummer.")
    except Exception as e:
        print(f"Error: {e}")
        return None

"""**This code defines a function called `extract_text_from_html` that takes HTML data as input and extracts the text content from it. Here is a summary explanation of the code:**

1. The function `extract_text_from_html` is defined with a single parameter, `html_data`, representing the HTML content of a webpage.

2. Inside the function, the `BeautifulSoup` class from the `bs4` module is used to parse the HTML data and create a BeautifulSoup object called `soup`.

3. The `BeautifulSoup` constructor takes two arguments: the `html_data` to parse and the parser to use. In this case, "html.parser" is specified as the parser.

4. The `soup.stripped_strings` property returns an iterator over all the strings in the parsed HTML data with leading and trailing whitespace removed.

5. A list comprehension is used to iterate over the `soup.stripped_strings` iterator and generate a list of stripped strings.

6. The list of stripped strings is then joined together using `' '.join()` with a space as the separator. This creates a single string where all the stripped strings are concatenated with spaces in between.

7. The resulting string, which represents the extracted text content from the HTML, is returned by the function.

Overall, this code utilizes the BeautifulSoup library to parse HTML data and extract the text content from it. By stripping leading and trailing whitespace from the extracted strings and joining them together, it produces a single string that represents the text content of the HTML.
"""

def extract_text_from_html(html_data):
    """HTML text extraction"""
    soup = BeautifulSoup(html_data, "html.parser")
    return ' '.join([text.strip() for text in soup.stripped_strings])

"""**This code defines a function called `extract_text_from_pdf` that takes a PDF file path as input and extracts the text content from the PDF. Here is a summary explanation of the code:**

1. The function `extract_text_from_pdf` is defined with a single parameter, `pdf_path`, representing the file path of the PDF.

2. Inside the function, an empty string variable called `text` is initialized to store the extracted text.

3. The `with` statement is used to open the PDF file in binary mode (`"rb"`).

4. The `PdfReader` class is used from the `PyPDF2` or `pdfreader` library to read the PDF file. The `PdfReader` is initialized with the opened file.

5. A loop is used to iterate over each page in the PDF. The loop runs from 0 to the total number of pages in the PDF.

6. For each page, the `extract_text()` method is called on the corresponding `reader.pages[page_number]` object. This method extracts the text content from the page as a string.

7. The extracted text from each page is appended to the `text` variable using the `+=` operator.

8. After iterating over all the pages, the function returns the extracted text content, which is stripped of leading and trailing whitespace using the `strip()` method.

Overall, this code uses the `PdfReader` class to read the PDF file, extract the text content from each page, and concatenate the extracted text into a single string. The resulting string represents the text content of the PDF file.
"""

def extract_text_from_pdf(pdf_path):
    """PDFs? No problem!."""
    text = ""
    with open(pdf_path, "rb") as file:
        reader = PdfReader(file)
        for page_number in range(len(reader.pages)):
            text += reader.pages[page_number].extract_text()
    return text.strip()

"""**This code defines a function called `download_pdf` that takes a URL as input, downloads a PDF file from that URL, and returns the temporary file path where the downloaded PDF is stored. Here is a summary explanation of the code:**

1. The function `download_pdf` is defined with a single parameter, `url`, representing the URL from which the PDF file should be downloaded.

2. Inside the function, a try-except block is used to handle any potential exceptions that may occur during the execution of the code.

3. The `requests.get(url, stream=True)` function is called to send an HTTP GET request to the specified URL with streaming enabled. Streaming allows the response content to be downloaded in chunks rather than loading the entire file into memory.

4. The code checks the status code of the response using `response.status_code`. If the status code is 200, it means the request was successful.

5. If the status code is 200, a temporary file is created using `tempfile.NamedTemporaryFile(delete=False)`. The `delete=False` argument ensures that the temporary file is not automatically deleted when closed.

6. The `with` statement is used to open the temporary file in write mode and assign it to the `temp_file` variable. This ensures that the file is closed properly after writing.

7. A loop is used to iterate over the response content in chunks. The `response.iter_content(chunk_size=8192)` method yields chunks of the response content with a specified chunk size of 8192 bytes.

8. Inside the loop, each chunk of content is written to the temporary file using the `temp_file.write(chunk)` statement.

9. After downloading all the content, the function returns the file path of the temporary file using `temp_file.name`.

10. If the status code is not 200, an error message is printed indicating that the URL couldn't be accessed.

11. If an exception occurs during the execution of the code (e.g., network error or invalid URL), the except block catches the exception, prints an error message, and returns `None`.

Overall, this code provides a way to download a PDF file from a given URL and store it in a temporary file. It handles different scenarios such as successful downloads, unsuccessful downloads, and exceptions. The temporary file path is then returned for further processing or usage.
"""

def download_pdf(url):
    """Downloads a PDF from a URL and returns the temporary file path."""
    try:
        response = requests.get(url, stream=True)
        if response.status_code == 200:
            temp_file = tempfile.NamedTemporaryFile(delete=False)
            with temp_file:
                for chunk in response.iter_content(chunk_size=8192):
                    temp_file.write(chunk)
            return temp_file.name
        else:
            print(f"Error {response.status_code}: Couldn't access {url}.")
            return None
    except Exception as e:
        print(f"Error: {e}")
        return None

"""**This code defines a function called `scrape_data` that takes a list of sources (URLs or PDF file paths) as input, scrapes the data from those sources, and returns a list of extracted text data. Here is a summary explanation of the code:**

1. The function `scrape_data` is defined with a single parameter, `sources`, representing a list of sources (URLs or PDF file paths) from which the data should be scraped.

2. Inside the function, an empty list variable called `data` is initialized to store the extracted text data.

3. A loop is used to iterate over each source in the `sources` list.

4. For each source, the code checks if it starts with `"http"`, indicating a URL. If it does, the code further checks if the source ends with `".pdf"`, indicating a PDF file.

5. If the source is a PDF file, the code prints a message indicating the PDF download is in progress and calls the `download_pdf` function to download the PDF from the source URL. The returned PDF file path is stored in the `pdf_path` variable.

6. If the PDF download is successful (i.e., `pdf_path` is not `None`), the code prints a message indicating the PDF text extraction is in progress and calls the `extract_text_from_pdf` function to extract the text content from the PDF. The extracted text is stored in the `text` variable.

7. The extracted text is appended to the `data` list.

8. The temporary PDF file is deleted using `os.unlink(pdf_path)` to remove the temporary file from the system.

9. If the PDF download is not successful (i.e., `pdf_path` is `None`), an error message is printed indicating that the PDF couldn't be downloaded.

10. If the source is an HTML URL (not a PDF), the code prints a message indicating the HTML scraping is in progress and calls the `get_html_data` function to retrieve the HTML data from the source URL. The returned HTML data is stored in the `html_data` variable.

11. If the HTML data retrieval is successful (i.e., `html_data` is not `None`), the code calls the `extract_text_from_html` function to extract the text content from the HTML. The extracted text is stored in the `text` variable.

12. The extracted text is appended to the `data` list.

13. If the source is neither a valid URL nor a PDF file path, an error message is printed.

14. After iterating over all the sources, the function returns the `data` list containing the extracted text data.

Overall, this code provides a way to scrape text data from various sources, including both URLs (HTML) and PDF files. It handles different scenarios such as downloading PDF files, extracting text from PDFs, retrieving HTML data, and handling invalid sources. The extracted text data is accumulated in a list and returned for further processing or usage.
"""

def scrape_data(sources):
    """The main attraction: throw me some sources and I'll scrape 'em for ya!"""
    data = []

    for source in sources:
        if source.startswith("http"):
            if source.endswith(".pdf"):
                print(f"Downloading PDF from {source}...")
                pdf_path = download_pdf(source)
                if pdf_path:
                    print(f"Extracting text from PDF {source}...")
                    text = extract_text_from_pdf(pdf_path)
                    data.append(text)
                    os.unlink(pdf_path)  # Delete the temporary file
                else:
                    print(f"Error: Couldn't download the PDF from {source}.")
            else:
                print(f"Scraping HTML from {source}...")
                html_data = get_html_data(source)
                if html_data:
                    text = extract_text_from_html(html_data)
                    data.append(text)
        else:
            print(f"Invalid source {source}. Please provide a valid URL or PDF file path.")
    
    return data

"""## **Data extraction**

In this phase of the script, we focus on extracting and organizing data from a predefined list of sources. The procedure is labeled as 'feature engineering' as it retrieves URLs and assigns labels to them.



---

**Initiate Data Extraction**

In the beginning, a list of web sources is declared. This list includes URLs to various HTML websites and paths to downloadable PDF files. These resources have been chosen as they contain important information about mortgage loans, which is the focus of this data scraping operation.
"""

# What are you waiting for? Let's scrape some data!
sources = [
    "https://finansdanmark.dk/en/the-association-of-danish-mortgage-banks/the-danish-mortgage-model/danish-mortgage-banking-in-practice/main-types-of-mortgage-loans/",
    "https://finansdanmark.dk/media/raim0hhz/den-klassiske-realkreditmodel_uk_2021_final.pdf",
    "https://bomae.dk/en/buyer-guide/danish-loans/",
    "https://coface-eu.org/wp-content/uploads/2021/12/COFACE-Report-SOCIAL-POLICY-AND-MORTGAGE-LENDING.pdf",
    "https://www.nationalbanken.dk/en/publications/Documents/2023/02/EM_buybacks_publication.pdf",
    "https://www.advodan.dk/en/private/service-areas/homes-and-rental/buying-a-home-in-denmark/buying-a-house-in-denmark/",
    "https://forenetkredit.dk/wp-content/uploads/2019/12/Banking-Union-and-the-Danish-mortgage-lending-model_TIL-OFFENTLIGG%C3%98RELSE.pdf",
    "https://jyskerealkredit.com/about/the-danish-mortgage-system/mortgage-legislation",
    "https://www.onlinemortgageadvisor.co.uk/overseas-mortgages/danish-mortgages/"
    "https://tjek-laan.dk/loans-in-denmark"


]

"""**Save the Output Function**

A function, save_output(scraped_data, output_file="output.txt"), is defined to write the scraped data into a text file. The function accepts a list of scraped data and an optional output file name. If no output file name is provided, it will default to output.txt.

**Main Execution Block**

Under the if __name__ == "__main__" conditional statement, which ensures the following block of code is run only when the script is executed directly (not imported as a module), the scrape_data(sources) function is called with the declared sources as its argument.

The scraped data, returned as a list of dictionaries, is then converted into a pandas DataFrame for easier data manipulation and analysis. The save_output(scraped_data) function is called to save the scraped data to a text file.
"""

def save_output(scraped_data, output_file="output.txt"):
    """Save the scraped data to a text file."""
    with open(output_file, "w", encoding="utf-8") as file:
        for i, text in enumerate(scraped_data):
            file.write(f"\nData from source {i + 1}:\n{text}\n")

if __name__ == "__main__":
    scraped_data = scrape_data(sources)
    save_output(scraped_data)
    print(f"Scraped data saved to output.txt")

if __name__ == "__main__":
    scraped_data = scrape_data(sources)
    for i, text in enumerate(scraped_data):
        print(f"\nData from source {i + 1}:\n{text}\n")

"""## **Text Preprocessing  (Spacy)**

This segment of the script is dedicated to preprocessing the text data obtained from the previous web scraping operation. It uses the Natural Language Toolkit (NLTK), a leading platform for building Python programs to work with human language data.

---

### **Making Sentences**

1. Importing the spaCy library:
   - `import spacy`: This line imports the spaCy library, which is a popular natural language processing (NLP) library.

2. Example list of sentences:
   - `sentences = scraped_data`: This line assigns the list of sentences, stored in the `scraped_data` variable, to the `sentences` variable. It assumes that `scraped_data` contains the list of sentences you want to process.

3. Convert the list into a single string:
   - `text = " ".join(sentences)`: This line joins the individual sentences from the `sentences` list into a single string by concatenating them with a space in between. The resulting string is assigned to the `text` variable.

4. Load the spaCy language model:
   - `nlp = spacy.load("en_core_web_sm")`: This line loads the spaCy language model for English, specifically the "en_core_web_sm" model. It initializes the `nlp` object, which represents the language processing pipeline.

5. Process the text with spaCy:
   - `doc = nlp(text)`: This line processes the `text` string using the spaCy language model. It applies various NLP techniques, such as tokenization, part-of-speech tagging, and dependency parsing, to analyze the text. The result is a `doc` object that represents the analyzed text.

6. Extract sentences from the analyzed text:
   - `sentences = [sent.text for sent in doc.sents]`: This line iterates over the sentences in the `doc` object and retrieves the text of each sentence using a list comprehension. It creates a new list of sentence strings, which are stored in the `sentences` variable.

The code assumes that you have installed spaCy and have the "en_core_web_sm" model downloaded before running the code. It demonstrates how to process a list of sentences using the spaCy library by converting the list into a single string, loading the language model, and extracting the sentences from the analyzed text.

The resulting `sentences` variable will contain the extracted sentences as individual strings.
"""

import spacy

# Example list of sentences
sentences = scraped_data

# Convert the list into a single string
text = " ".join(sentences)

# Load the spaCy language model
nlp = spacy.load("en_core_web_sm")

# Process the text with spaCy
doc = nlp(text)
sentences = [sent.text for sent in doc.sents]
print(sentences)

"""### **Making DataFrame**"""

import pandas as pd

df = pd.DataFrame({'Sentences': sentences})

df

df.to_csv('output.csv', index=False)

"""### **Cleaning the Sentences**

**1. `remove_patterns(text)`: This function removes patterns related to numbers, time, date, year, and month from a given sentence. Here's what each step does:**
   - `re.sub(r'\d+', '', text)`: Removes any sequence of digits from the sentence.
   - `re.sub(r'\d{1,2}:\d{2}\s?(?:AM|PM)?', '', text)`: Removes time expressions in the format of "H:MM" or "H:MM AM/PM".
   - `re.sub(r'\d{1,2}/\d{1,2}/\d{2,4}', '', text)`: Removes date formats in the format of "DD/MM/YYYY" or "M/D/YY".
   - `re.sub(r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2}(?:st|nd|rd|th)?(?:,)?\s+\d{2,4}', '', text)`: Removes date formats with month names, such as "Month DD, YYYY" or "Month DDth, YYYY".
   - `re.sub(r'\b(?:19|20)\d{2}\b', '', text)`: Removes year numbers in the format of "YYYY", considering only years starting with "19" or "20".
   - `re.sub(r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\b', '', text)`: Removes month names from the sentence.

**2. Applying the function to the DataFrame:**
   - `df['Processed_Sentence'] = df['Sentences'].apply(remove_patterns)`: This line applies the `remove_patterns` function to each sentence in the 'Sentences' column of the DataFrame.
   - The processed sentences are stored in a new column named 'Processed_Sentence'.

The code assumes that you have a DataFrame named `df` with a column named 'Sentences' that contains sentences. The `remove_patterns` function is applied to each sentence in the 'Sentences' column, and the processed sentences are stored in a new column named 'Processed_Sentence'.

The resulting DataFrame will have the original 'Sentences' column and the newly added 'Processed_Sentence' column, which contains the sentences with the specified patterns removed.
"""

import pandas as pd
import re

# Function to remove numbers, time, date, year, and month from a sentence
def remove_patterns(text):
    # Remove numbers
    text = re.sub(r'\d+', '', text)
    
    # Remove time (e.g., 2:30 PM)
    text = re.sub(r'\d{1,2}:\d{2}\s?(?:AM|PM)?', '', text)
    
    # Remove dates (e.g., 22/05/2023, May 22, 2023)
    text = re.sub(r'\d{1,2}/\d{1,2}/\d{2,4}', '', text)
    text = re.sub(r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2}(?:st|nd|rd|th)?(?:,)?\s+\d{2,4}', '', text)
    
    # Remove year (e.g., 1990)
    text = re.sub(r'\b(?:19|20)\d{2}\b', '', text)
    
    # Remove month names
    text = re.sub(r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\b', '', text)
    
    return text

# Apply the function to the 'Sentence' column of the DataFrame
df['Processed_Sentence'] = df['Sentences'].apply(remove_patterns)

# Print the updated DataFrame
print(df)

"""**The given code drops a single column named 'Sentences' from the DataFrame `df` and then prints the updated DataFrame. Here's a summary explanation of the code:**

1. The `drop()` method is called on the DataFrame `df` to drop a column. The first argument passed to `drop()` is the name of the column to be dropped, which is 'Sentences' in this case.

2. The `axis=1` parameter is provided to specify that the column should be dropped (as opposed to dropping a row, which would be indicated by `axis=0`).

3. The `drop()` method returns a new DataFrame with the specified column dropped, but the original DataFrame `df` remains unchanged.

Overall, this code drops the 'Sentences' column from the DataFrame `df` and displays the resulting DataFrame.
"""



# Drop a single column
df = df.drop('Sentences', axis=1)

# Print the updated DataFrame
print(df)

import pandas as pd

# Remove specific characters from the 'Sentence' column
df['Text'] = df['Processed_Sentence'].str.replace(r'[\n!@#$%^*()\[\]{}-]', '', regex=True)

# Print the updated DataFrame
print(df)

# Drop a single column
df = df.drop('Processed_Sentence', axis=1)

# Print the updated DataFrame
print(df)

"""**The code demonstrates how to drop rows from a DataFrame where the 'Text' column contains sentences with consecutive single letters. Here's the documentation:**

**Drop Sentences with Consecutive Single Letters:**
- `df[~df['Text'].str.contains(r'\b[a-zA-Z]\s+[a-zA-Z]\b')]`: This line filters the DataFrame, dropping rows where the 'Text' column contains sentences with consecutive single letters.
  - The `str.contains()` method is used to check if the 'Text' column contains the specified pattern.
  - The regular expression pattern `r'\b[a-zA-Z]\s+[a-zA-Z]\b'` matches consecutive single letters separated by whitespace.
  - The `~` operator negates the condition, selecting rows where the pattern is not found.
  - The resulting DataFrame without the dropped rows is assigned back to `df`.

**Reset DataFrame Index:**
- `df = df.reset_index(drop=True)`: This line resets the index of the DataFrame after dropping the rows. The `reset_index()` method is used with the `drop=True` parameter to reset the index and remove the old index column.

The code assumes that you have a DataFrame named `df` with a column named 'Text' containing sentences. The code drops rows where the sentences in the 'Text' column contain consecutive single letters.

You can use this code by providing the appropriate DataFrame and running the code. The output will be the updated DataFrame with the specified rows dropped and the index reset.
"""

# Drop sentences containing '.com' or '.dk' from the 'Sentence' column
df = df[~df['Text'].str.contains(r'\.com|\.dk')]

# Reset the index of the DataFrame
df = df.reset_index(drop=True)

# Print the updated DataFrame
print(df)

"""**The code includes two functions: `has_consecutive_single_words` and `remove_consecutive_single_words`. It checks for consecutive single words in a sentence, removes them, and updates the DataFrame. Here's the documentation:**

**Function to Check for Consecutive Single Words:**
- `has_consecutive_single_words(text)`: This function takes a single parameter, `text`, representing the input sentence.
- The function splits the sentence into words and iterates through the words using a `for` loop.
- It checks if two consecutive words have a length of 1 character using `len(words[i]) == 1` and `len(words[i+1]) == 1`.
- If consecutive single words are found, the function returns `True`. Otherwise, it returns `False`.

**Drop Rows with Consecutive Single Words:**
- `df[~df['Text'].apply(has_consecutive_single_words)]`: This line filters the DataFrame, dropping rows where the 'Text' column contains sentences with consecutive single words.
  - The `apply()` method applies the `has_consecutive_single_words` function to each row in the 'Text' column.
  - The `~` operator negates the condition, selecting rows where the function returns `False`.
  - The resulting DataFrame without the dropped rows is assigned back to `df`.

**Function to Remove Consecutive Single Words:**
- `remove_consecutive_single_words(text)`: This function takes a single parameter, `text`, representing the input sentence.
- The function splits the sentence into words and uses a list comprehension to create a new list of words that have a length greater than 1 character.
- It joins the filtered words back into a single string using `' '.join(...)`.
- The resulting sentence with consecutive single words removed is returned.

**Remove Consecutive Single Words from the 'Sentence' Column:**
- `df['Clean_text'] = df['Text'].apply(remove_consecutive_single_words)`: This line applies the `remove_consecutive_single_words` function to each sentence in the 'Text' column of the
"""

# Drop sentences with consecutive single letters from the 'Sentence' column
df = df[~df['Text'].str.contains(r'\b[a-zA-Z]\s+[a-zA-Z]\b')]

# Reset the index of the DataFrame
df = df.reset_index(drop=True)

# Print the updated DataFrame
print(df)

"""**1. `has_consecutive_single_words(text)`:** 
This function checks if a sentence contains consecutive single words. It splits the sentence into words and iterates through the words using a `for` loop. If two consecutive words have a length of 1 character, it returns `True`. Otherwise, it returns `False`.

**2. Drop rows with consecutive single words from the 'Sentence' column:**
   - `df = df[~df['Text'].apply(has_consecutive_single_words)]`: This line filters the DataFrame, keeping only the rows where the 'Text' column does not have consecutive single words. It applies the `has_consecutive_single_words` function to each row in the 'Text' column using the `apply` method and negates the result with `~`.

**3. `remove_consecutive_single_words(text)`:**
This function removes consecutive single words from a sentence. It splits the sentence into words and uses a list comprehension to create a new list of words where each word has a length greater than 1 character. It then joins the filtered words back into a single string.

**4. Remove consecutive single words from the 'Sentence' column:**
   - `df['Clean_text'] = df['Text'].apply(remove_consecutive_single_words)`: This line applies the `remove_consecutive_single_words` function to each sentence in the 'Text' column of the DataFrame. It creates a new column called 'Clean_text' and assigns the processed sentences to it.

**5. Reset the index of the DataFrame:**
   - `df = df.reset_index(drop=True)`: This line resets the index of the DataFrame after removing rows with consecutive single words. The `reset_index` method is used with the `drop=True` parameter to reset the index and remove the old index column.

The code assumes that you have a DataFrame named `df` with a column named 'Text' containing sentences. It drops rows where the sentences in the 'Text' column have consecutive single words, creates a new column 'Clean_text' with the sentences where consecutive single words are removed, and resets the index of the DataFrame.

The resulting DataFrame will have the rows with consecutive single words removed and the index reset.
"""

# Function to check for consecutive single words
def has_consecutive_single_words(text):
    words = text.split()
    for i in range(len(words) - 1):
        if len(words[i]) == 1 and len(words[i+1]) == 1:
            return True
    return False

# Drop rows with consecutive single words from the 'Sentence' column
df = df[~df['Text'].apply(has_consecutive_single_words)]

# Function to remove consecutive single words
def remove_consecutive_single_words(text):
    return ' '.join([word for word in text.split() if len(word) > 1])

# Remove consecutive single words from the 'Sentence' column
df['Clean_text'] = df['Text'].apply(remove_consecutive_single_words)

# Reset the index of the DataFrame
df = df.reset_index(drop=True)

# Print the updated DataFrame
print(df)

# Drop a single column
df = df.drop('Text', axis=1)

"""**1. Calculate the character length of each sentence:**
   - `df['Character Length'] = df['Clean_text'].str.len()`: This line calculates the character length of each sentence in the 'Clean_text' column of the DataFrame and stores the result in a new column named 'Character Length'.
   - The `str.len()` method is used to calculate the length of each string in the 'Clean_text' column.

**2. Calculate the word length of each sentence:**
   - `df['Word Length'] = df['Clean_text'].str.split().str.len()`: This line calculates the word length of each sentence in the 'Clean_text' column of the DataFrame and stores the result in a new column named 'Word Length'.
   - The `str.split()` method is used to split each sentence into a list of words.
   - The `str.len()` method is then applied to the resulting list of words to calculate the word length of each sentence.

The code assumes that you have a DataFrame named `df` with a column named 'Clean_text' that contains the processed sentences. By applying the respective methods, the code calculates and adds two new columns ('Character Length' and 'Word Length') to the DataFrame, representing the character length and word length of each sentence, respectively.

The resulting DataFrame will have the original columns along with the newly added 'Character Length' and 'Word Length' columns.
"""

# Calculate the character length of each sentence
df['Character Length'] = df['Clean_text'].str.len()

# Calculate the word length of each sentence
df['Word Length'] = df['Clean_text'].str.split().str.len()

# Print the updated DataFrame
print(df)

# Drop sentences with less than four words from the 'Sentence' column
df = df[df['Clean_text'].str.split().str.len() >= 4]

# Reset the index of the DataFrame
df = df.reset_index(drop=True)

# Print the updated DataFrame
print(df)

"""**1. Drop sentences with URLs from the 'Sentence' column:**
   - `df = df[~df['Clean_text'].str.contains(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')]`: This line filters the DataFrame, dropping rows where the 'Clean_text' column contains sentences with URLs.
   - The `str.contains()` method is used to check if the 'Clean_text' column contains the specified regular expression pattern for URLs.
   - The regular expression pattern `r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'` matches URLs of various formats.
   - The `~` operator negates the condition, selecting rows where the pattern is not found.
   - The resulting DataFrame without the dropped rows is assigned back to `df`.

**2. Reset the index of the DataFrame:**
   - `df = df.reset_index(drop=True)`: This line resets the index of the DataFrame after dropping the rows. The `reset_index()` method is used with the `drop=True` parameter to reset the index and remove the old index column.

The code assumes that you have a DataFrame named `df` with a column named 'Clean_text' that contains the processed sentences. The code drops rows where the sentences in the 'Clean_text' column contain URLs, and then resets the index of the DataFrame.

The resulting DataFrame will have the original columns, but with the rows containing sentences with URLs removed and the index reset.
"""

# Drop sentences with URLs from the 'Sentence' column
df = df[~df['Clean_text'].str.contains(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')]

# Reset the index of the DataFrame
df = df.reset_index(drop=True)

df

"""## **Open AI**

**1. Set the locale to UTF-8:**
   - `os.environ['LC_ALL'] = 'en_US.UTF-8'`: This line sets the value of the 'LC_ALL' environment variable to 'en_US.UTF-8'. The 'LC_ALL' variable controls the locale for all aspects of the user interface, including character encoding.
   - `os.environ['LANG'] = 'en_US.UTF-8'`: This line sets the value of the 'LANG' environment variable to 'en_US.UTF-8'. The 'LANG' variable specifies the default locale for the user's shell session.
   - `os.environ['LANGUAGE'] = 'en_US.UTF-8'`: This line sets the value of the 'LANGUAGE' environment variable to 'en_US.UTF-8'. The 'LANGUAGE' variable determines the language preference for the user's environment.

By setting the locale to UTF-8, you are ensuring that the system's encoding and language settings are configured to support UTF-8, which is a character encoding that can represent a wide range of characters from different languages and scripts.

The code uses the `os` module to access and modify environment variables. Setting the locale to UTF-8 is important for handling and displaying text data correctly, especially when working with multilingual or non-ASCII characters.
"""

import os

# Set the locale to UTF-8
os.environ['LC_ALL'] = 'en_US.UTF-8'
os.environ['LANG'] = 'en_US.UTF-8'
os.environ['LANGUAGE'] = 'en_US.UTF-8'

"""**1. Importing the `locale` module:**
   - `import locale`: This line imports the `locale` module, which provides access to various locale-related functionalities.

**2. Defining the `getpreferredencoding` function:**
   - `def getpreferredencoding(do_setlocale=True):`: This line defines a function named `getpreferredencoding` that takes an optional boolean parameter `do_setlocale` with a default value of `True`.
   - The function returns the string `"UTF-8"` as the preferred encoding.

**3. Overriding the `locale.getpreferredencoding` function:**
   - `locale.getpreferredencoding = getpreferredencoding`: This line assigns the `getpreferredencoding` function to the `getpreferredencoding` attribute of the `locale` module.
   - By doing this, the original functionality of `locale.getpreferredencoding` is replaced with the custom implementation defined in the `getpreferredencoding` function.

The code essentially overrides the behavior of `locale.getpreferredencoding` and sets it to always return the string `"UTF-8"`. The preferred encoding is a system-dependent setting that specifies the default character encoding for the current locale. By overriding it to return `"UTF-8"`, the code ensures that the preferred encoding is consistently set to UTF-8, regardless of the actual system settings.

This can be useful when working with text data that is encoded in UTF-8 and you want to ensure consistent handling and processing of Unicode characters.
"""

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

"""**Installing all the required !pips**"""

# Install the openai package
!pip install --no-cache-dir openai -qq

# Install the openai package
!pip install openai -qq

# Install the openai package with LANG set to en_US.UTF-8
!LANG=en_US.UTF-8 pip install openai -qq

"""**Importing OPEN AI and installing the API KEY**"""

import openai

OPENAI_KEY= "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"

import os

openai_key = os.getenv('OPENAI_KEY')

from openai.embeddings_utils import get_embedding

"""**1. `get_text_embedding(text)`:** This function takes a `text` parameter, representing the input text for which you want to generate an embedding.

**2. Generating text embedding using OpenAI API:**
   - `response = openai.Completion.create(...)`: This line sends a request to the OpenAI API for text completion using the specified parameters.
   - The `engine` parameter is set to "text-embedding-ada-002" to use the Text Embedding model.
   - The `prompt` parameter is set to the input `text` for which you want to generate an embedding.
   - The `max_tokens` parameter is set to 1, indicating that you only want to generate a single token.
   - The `echo` parameter is set to True, which causes the input prompt to be returned in the response.
   - The `logprobs` parameter is set to 0, indicating that you don't need the log probabilities of the generated tokens.

**3. Extracting the embedding from the response:**
   - `if response.choices and response.choices[0].logprobs:`: This line checks if the response has choices and if the first choice has log probabilities.
   - `embedding = response.choices[0].logprobs.token_logprobs[0]`: This line extracts the embedding from the log probabilities of the first choice and assigns it to the `embedding` variable.

**4. Returning the embedding:**
   - `return embedding`: This line returns the extracted embedding if it exists.

**5. Handling the case where no embedding is available:**
   - `else: return None`: If the response does not have choices or log probabilities, or if the embedding extraction fails, the function returns `None`.

The code assumes that you have the OpenAI package installed and have the necessary authentication and access to the OpenAI API. It uses the OpenAI API to generate a text embedding for the input `text` by making a completion request to the Text Embedding model.
"""

# Set your OpenAI API key
openai.api_key = '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'

def get_text_embedding(text):
    response = openai.Completion.create(
        engine="text-embedding-ada-002",
        prompt=text,
        max_tokens=1,
        echo=True,
        logprobs=0
    )
    if response.choices and response.choices[0].logprobs:
        embedding = response.choices[0].logprobs.token_logprobs[0]
        return embedding
    else:
        return None

"""**The given code creates an empty list named `embeddings` which is intended to store embeddings. Here's a summary explanation of the code:**

1. The code initializes an empty list named `embeddings` using empty brackets `[]`.

2. The purpose of this list is to store embeddings, which are typically vector representations of data points in a high-dimensional space.

3. This empty list can be populated with embeddings obtained from some other process or algorithm.

4. The list `embeddings` can be later used for further analysis, processing, or any other operations required for the specific use case.

Overall, this code sets up an empty list `embeddings` to store embeddings, providing a starting point for collecting and manipulating embedding data.
"""

#Create an empty list to store the embeddings
embeddings = []

"""**The function `get_embedding` used in the code provided is defined for embeddings. The intention is to apply a function called `get_embedding` to each element in the 'Clean_text' column of the DataFrame `df`. The resulting embeddings will be stored in a new column called 'embedding'.**"""

#generating the embeddings
df['embedding'] = df['Clean_text'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))

df

df.to_csv('Embedded.csv', index=False)

"""## Clustering on Embeddings

This section deal with performing clustering on the sentence embeddings and visualizes the results. This is also what we have been refering to our "EDA" part as it highlights the quality of our data before we go further ahead.

We start by getting the necessary Python libraries for the script. The libraries include seaborn (for data visualization), matplotlib (for creating figures and plots), pandas (for data manipulation and analysis), and scikit-learn (for machine learning and data mining).
"""

!pip install seaborn matplotlib pandas scikit-learn -qq

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
from sklearn.metrics import pairwise_distances_argmin_min
from sklearn.metrics import pairwise_distances
import matplotlib.pyplot as plt

"""We designate the sentences_df, the one containing the pre-processed sentences as the we will be refering to

Using scaler, normalize the sentence embeddings using StandardScaler from scikit-learn. Normalization is important in machine learning as it standardizes the range of input features, ensuring they are on a similar scale.
"""

# Normalize the embeddings using StandardScaler
scaler = StandardScaler()
scaled_embeddings = scaler.fit_transform(list(df['embedding']))

"""Principal Component Analysis (PCA) is a dimensionality reduction technique that is often used to visualize high-dimensional data. Here, PCA is performed on the normalized sentence embeddings, and the results are stored in new columns of the dataframe ('pca-one' and 'pca-two')."""

# Perform PCA for better visualization
pca = PCA(n_components=2)
pca_result = pca.fit_transform(scaled_embeddings)

df['pca-one'] = pca_result[:,0]
df['pca-two'] = pca_result[:,1]

"""And now we plot the PCA results using seaborn's scatterplot function, visualizing the distribution of sentence embeddings in a 2D space."""

# Visualize the PCA result
plt.figure(figsize=(10,10))
sns.scatterplot(
    x="pca-one", y="pca-two",
    palette=sns.color_palette("hls", 10),
    data=df,
    legend="full",
    alpha=0.6
)

"""So right now it looks like a mess, so we need to put some labels on the graph to visualize it better

KMeans clustering, as we use here, is performed on the normalized sentence embeddings. This unsupervised learning method partitions the embeddings into 3 clusters. The resulting cluster labels are then added to the dataframe
"""

# KMeans clustering
kmeans = KMeans(n_clusters=3)  # Change the number of clusters as needed
kmeans.fit(scaled_embeddings)
df['cluster'] = kmeans.labels_

"""The clusters are visualized using another scatterplot, this time with different colors representing the different clusters."""

# Visualize the clusters
plt.figure(figsize=(10,10))
sns.scatterplot(
    x="pca-one", y="pca-two",
    hue="cluster",
    palette=sns.color_palette("hls", 3),  # Change the palette as needed
    data=df,
    legend="full",
    alpha=0.6
)

"""As we see, the clusters are spread across three cluster groups

We now setup the TF-IDF vectorizer. However, the vectorizer is not used in the following code.
"""

# Initialize TF-IDF vectorizer
vectorizer = TfidfVectorizer(stop_words='english', max_features=2000)

"""We also designate a empty dictionary to store the resulting dataframes of top sentences for each cluster."""

# Create an empty dictionary to store the cluster dataframes
cluster_dataframes = {}

"""Setting up a loop over each cluster, we calculate the Euclidean distance between each sentence in a cluster and the cluster's center, and select the indices of the top 10 sentences closest to the center."""

# Find the most representative sentences in each cluster
for i in range(3):  # Number of clusters
    cluster_data = df[df['cluster'] == i].reset_index()  # Reset index for correct indexing
    distances = pairwise_distances(kmeans.cluster_centers_[i].reshape(1, -1), scaled_embeddings[cluster_data.index])
    
    # Get indices of top 10 closest sentences
    top_sentences_indices = distances.argsort()[0][:10]
    
    top_sentences = cluster_data.loc[top_sentences_indices, 'Clean_text']  # Assuming 'Clean_text' is the column containing the text data
    cluster_dataframes[i] = top_sentences.to_frame()  # Save the sentences to a new dataframe

"""For each cluster, we print out the top 10 sentences along with the cluster number. The sentences are stored in separate dataframes, and each dataframe is saved in the cluster_dataframes dictionary."""

for i in range(3):
    print(f"Top 10 Sentences for Cluster {i}:\n")
    print(cluster_dataframes[i].reset_index(drop=True))  # Reset the index for better visualization
    print("\n---------------------\n")

"""As we see, Cluster 0 and Cluster 2 is poninting in the direction it is unwarranted/irrelevant data for the intentions of our project due to the top-10 features for both respective clusters being data that is not in the scope of our data-mining"""

cluster_counts = df['cluster'].value_counts()

# Plotting the distribution
cluster_counts.plot(kind='bar')
plt.xlabel('Cluster ID')
plt.ylabel('Number of Data Points')
plt.title('Distribution of Data Points in Clusters')
plt.show()

"""Fortunately, Cluster 1 with about 700 sentences. The cluster which displayed the most relevant, and contextually good results had the most sentences, whereas cluster 0 and 2 was less represented. This still confers that approximately 600 +/- of the sentences for the Danish data may not be applicable, however we will speak about that further when adressing the constraints of data-mining in our project discussion and cluster findings.

## **Pinecone**

**Shaping the Data for Pinecone Index**
"""

!pip install pinecone-client -qq

# Drop a single column
df = df.drop('Character Length', axis=1)

# Drop a single column
df = df.drop('Word Length', axis=1)

# Assign unique IDs as strings to each sentence
df['ID'] = df.reset_index().index.astype(str)

df

"""**The given code utilizes the OpenAI Embeddings API to calculate cosine similarity between a search sentence and the embeddings stored in a DataFrame column. Here's a summary explanation of the code:**

1. The code prompts the user to enter a search sentence using the `input()` function. The entered sentence is assigned to the variable `sentences_search`.

2. The `get_embedding()` function is called with `sentences_search` as the input sentence to retrieve the embedding vector representation for the search sentence. The `engine="text-embedding-ada-002"` argument specifies the specific engine to use for generating the embedding vector.

3. The obtained embedding vector for the search sentence is assigned to the variable `sentences_search_vector`.

4. The code adds a new column named "similarities" to the DataFrame `df` using `df["similarities"] = ...`.

5. The `.apply()` method is called on the "embedding" column of `df` to calculate cosine similarity between each embedding vector in the column and the `sentences_search_vector`. The `cosine_similarity()` function from the `openai.embeddings_utils` module is used for this purpose.

6. The calculated cosine similarities are assigned to the "similarities" column for each corresponding row in `df`.

Overall, this code enables searching for similar sentences in the DataFrame `df` based on cosine similarity between their embeddings and the search sentence's embedding. The cosine similarities are stored in a new "similarities" column within `df`.
"""

from openai.embeddings_utils import cosine_similarity

sentences_search = input("Search sentences for a sentence:")

sentences_search_vector = get_embedding(sentences_search, engine="text-embedding-ada-002")
sentences_search_vector

df["similarities"] = df['embedding'].apply(lambda x: cosine_similarity(x, sentences_search_vector))

df

# Assuming you have a dataframe named 'df' with a column 'similarity' containing similarity values

# Find the index of the row with the highest similarity value
max_similarity_index = df['similarities'].idxmax()

# Retrieve the corresponding row or specific columns from the row with the highest similarity value
row_with_max_similarity = df.loc[max_similarity_index]
# Alternatively, you can retrieve specific columns using:
# max_similarity_value = df.loc[max_similarity_index, 'similarity']

# Print the row or specific columns
print(row_with_max_similarity)
# Alternatively, print specific columns:
# print(max_similarity_value)

# Calculate the average similarity
average_similarity = df['similarities'].mean()

# Print the average similarity
print("Average Similarity:", average_similarity)

"""**The given code demonstrates the usage of the Pinecone API for working with vector indexes. Here's a summary explanation of the code:**

1. The code imports the `pinecone` module, which provides the necessary functions and classes for interacting with the Pinecone service.

2. The `pinecone.init()` function is called to initialize the connection to the Pinecone service. It requires an API key, which can be obtained from the Pinecone website (app.pinecone.io). Additionally, the `environment` parameter specifies the deployment environment (in this case, "us-east1-gcp").

3. The code checks if an index named 'finance' already exists in the Pinecone service using the `pinecone.list_indexes()` function. If the 'finance' index does not exist, the code creates it using the `pinecone.create_index()` function. The `dimension=1536` argument specifies the dimensionality of the vectors that will be stored in the index.

4. The code establishes a connection to the 'finance' index using the `pinecone.Index()` class. The `index` object can be used to perform various operations on the index, such as adding vectors, querying for nearest neighbors, and more.

Overall, this code sets up a connection to the Pinecone service, creates an index (if it doesn't exist already) with a specified dimension, and establishes a connection to that index for further operations. It demonstrates the basic initialization and setup process for using Pinecone's vector index service.
"""

import pinecone

# initialize connection to pinecone (get API key at app.pinecone.io)
pinecone.init(
    api_key="47cb1de2-5bd4-4166-b4e8-bb7ae9db8393",
    environment="us-east1-gcp"
)
# check if 'openai' index already exists (only create index if not)
if 'finance' not in pinecone.list_indexes():
    pinecone.create_index('finance', dimension=1536)
# connect to index
index = pinecone.Index('finance')

"""**The given code prepares a list of dictionaries to perform a bulk upsert operation in Pinecone. Here's a summary explanation of the code:**

1. The code imports the `numpy` library as `np` and the `ast` module.

2. The 'Clean_text' column of the DataFrame `df` is converted to a string type using the `astype()` method and assigned back to the 'Clean_text' column of `df`.

3. An empty list named `pinecone_index` is initialized to store the dictionaries for bulk upsert.

4. A loop is used to iterate over each row in the DataFrame `df` using the `iterrows()` method.

5. For each row, the embedding is retrieved from the 'embedding' column as a NumPy array using `np.array(row['embedding'])`.

6. The ID, embedding, and 'Clean_text' values are converted to strings and combined into a tuple named `document`. The ID is accessed using `str(row["ID"])`, the embedding is converted to a string using `str(embedding)`, and the 'Clean_text' is converted to a string within a dictionary using `str({"text": row["Clean_text"]})`.

7. The `document` tuple is appended to the `pinecone_index` list.

Overall, this code converts the necessary values to string types and prepares a list of dictionaries that can be used for bulk upsert operations in Pinecone. Each dictionary in the list represents a document and contains the ID, embedding, and 'Clean_text' value for that document.
"""

import numpy as np
import ast

# Convert Clean_text to string type
df['Clean_text'] = df['Clean_text'].astype(str)


# Prepare a list of dictionaries for bulk upsert
pinecone_index = []
for _, row in df.iterrows():
    embedding = np.array(row['embedding'])
    document = (str(row["ID"]), str(embedding), str({"text":row["Clean_text"]}))

    pinecone_index.append(document)

# # Define the batch size for upsert operations
# batch_size = 100

# # Perform batched upsert operations
# index = pinecone.Index('finance')
# num_documents = len(pinecone_index)
# num_batches = (num_documents + batch_size - 1) // batch_size
# for i in range(num_batches):
#     start = i * batch_size
#     end = (i + 1) * batch_size
#     batch = pinecone_index[start:end]
#     index.upsert(batch)

print(pinecone_index)

"""**The given code defines a function called `format_list_entries` that takes an input list as input and formats its entries. Here's a summary explanation of the code:**

1. The function `format_list_entries` is defined with a single parameter, `input_list`, representing the list that needs to be formatted.

2. An empty list named `formatted_list` is initialized to store the formatted entries.

3. A loop is used to iterate over each entry in the `input_list`.

4. For each entry, the code assumes that the entry is a list with three elements. The first element is converted to an integer using `int(entry[0])` and assigned to `first_entry`. The second and third elements are assigned to `second_entry` and `third_entry`, respectively.

5. The formatted entry is created as a tuple using `(first_entry, second_entry, third_entry)` and assigned to `formatted_entry`.

6. The `formatted_entry` is appended to the `formatted_list`.

7. After iterating over all the entries, the function returns the `formatted_list` containing the formatted entries.

8. The code calls the `format_list_entries` function with `pinecone_index` as the input and assigns the returned formatted list to `formatted_list`.

9. A loop is used to iterate over each entry in the `formatted_list`.

10. For each entry, the code prints the entry using the `print()` function.

Overall, this code takes an input list, assumes each entry has three elements, formats the entries by converting the first element to an integer and keeps the second and third elements as they are. The formatted entries are stored in a new list named `formatted_list`, which is then printed using a loop.
"""

def format_list_entries(input_list):
    formatted_list = []
    
    for entry in input_list:
        first_entry = int(entry[0])
        second_entry = entry[1]
        third_entry = entry[2]
        
        formatted_entry = (first_entry, second_entry, third_entry)
        formatted_list.append(formatted_entry)
    
    return formatted_list

formatted_list = format_list_entries(pinecone_index)

# Printing the formatted list entries
for entry in formatted_list:
    print(entry)

"""**The modified code prepares a list of dictionaries for bulk upsert, based on the DataFrame `df`. Here's a summary explanation of the modified code:**

1. An empty list named `pinecone_index` is initialized to store the dictionaries for bulk upsert.

2. A loop is used to iterate over each row in the DataFrame `df` using the `iterrows()` method.

3. For each row, the ID is incremented by 3000 using `ID = f"{int(row['ID'])+3000}"`. This ensures that the IDs in the Pinecone index are different from the original IDs in the DataFrame.

4. The embedding and sentence values are directly assigned from the 'embedding' and 'Clean_text' columns, respectively.

5. The metadata dictionary is created with the 'text' key set to the value of the 'Clean_text' column using `metadata = {"text": sentence}`.

6. The `document` tuple is created with the ID, embedding, and metadata as its elements.

7. The `document` tuple is appended to the `pinecone_index` list.

Overall, this modified code prepares a list of dictionaries for bulk upsert in Pinecone, with incremented IDs and the necessary values from the DataFrame `df`. The list of dictionaries, `pinecone_index`, can be used for performing bulk upsert operations in Pinecone.
"""

# Prepare a list of dictionaries for bulk upsert
pinecone_index = []
for _, row in df.iterrows():
    ID = f"{int(row['ID'])+3000}"
    embedding = row['embedding']
    sentence = row['Clean_text']
    metadata = {"text": sentence}
    document = (
        ID,
        embedding,
        metadata
    )
    pinecone_index.append(document)

print(pinecone_index[0])

"""**The given code performs batched upsert operations in Pinecone, using a specified batch size. Here's a summary explanation of the code:**

1. The variable `batch_size` is defined to determine the number of documents to include in each batch.

2. The variable `num_documents` is assigned the length of the `pinecone_index` list, which represents the total number of documents to be upserted.

3. The variable `num_batches` calculates the total number of batches required to cover all the documents. The expression `(num_documents + batch_size - 1) // batch_size` performs integer division and rounds up any remaining fraction to the next whole number.

4. A loop is used to iterate over each batch, using the range of `num_batches` as the iterator.

5. For each iteration, the variables `start` and `end` are calculated to slice the `pinecone_index` list and retrieve the documents for the current batch. The `start` index is set to `i * batch_size`, and the `end` index is set to `(i + 1) * batch_size`.

6. The `batch` variable is assigned the slice of `pinecone_index` containing the documents for the current batch.

7. The `#index.upsert(batch)` line is commented out, but this is where the actual upsert operation would be performed. Depending on the specific implementation and Pinecone library being used, this line would typically invoke the `upsert()` method of the Pinecone index object to perform the batched upsert operation.

Overall, this code demonstrates a basic framework for performing batched upsert operations in Pinecone. It calculates the number of batches based on the desired batch size and the total number of documents, and then iterates over each batch to process and upsert the documents. The specific upsert operation is not executed in the provided code, but it is typically performed within the commented line where `#index.upsert(batch)` is indicated.
"""

# Perform batched upsert operations
batch_size = 100
num_documents = len(pinecone_index)
num_batches = (num_documents + batch_size - 1) // batch_size

for i in range(num_batches):
    start = i * batch_size
    end = (i + 1) * batch_size
    batch = pinecone_index[start:end]
    #index.upsert(batch)

"""**The `index.describe_index_stats()` method provides descriptive statistics about the Pinecone index. It returns information such as the total number of vectors in the index, the index dimensionality, and other relevant statistics. Here's an example usage of the method:**

The `index_stats` variable will contain the statistics returned by the `describe_index_stats()` method. You can then print or access specific information from the `index_stats` object as needed.

Please note that the `describe_index_stats()` method assumes that the `index` variable refers to a valid Pinecone index object. Make sure you have a valid index object instantiated and connected before calling this method.
"""

index.describe_index_stats()

"""# **Constructing our proof-of-concept Agent**

## **Setting up Vector Store**

**Installing Necessary Libraries and API Keys for Langchain**
"""

!pip install langchain -qq
!pip show cred -qq
!pip install cred -qq

import sys
sys.path.append('/path/to/cred/module')

import cred

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Pinecone
from langchain.document_loaders import TextLoader

import openai
import pinecone

from langchain.embeddings.openai import OpenAIEmbeddings

openai.api_key = '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'
OPENAI_API_TOKEN = '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'

import os 
os.environ["OPENAI_API_KEY"] = "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"

embeddings = OpenAIEmbeddings()

"""**The provided code demonstrates how to utilize the `langchain.vectorstores.Pinecone` class for similarity search using an existing Pinecone index. Here's a summary explanation of the code:**

1. The `langchain.vectorstores.Pinecone` class is imported, assuming it is part of the "langchain" library.

2. The variable `index_name` is set to the name of the existing Pinecone index you want to load.

3. An instance of the `Pinecone` class is created by calling `Pinecone.from_existing_index(index_name, embeddings)`. This initializes the `docsearch` object to perform similarity search using the specified index.

4. The variable `query` is set to the query sentence or text for which you want to find similar documents.

5. The `similarity_search(query)` method is called on the `docsearch` object to perform the similarity search. The search is executed against the loaded index, and the results are stored in the `docs` variable.

6. The `docs` variable holds the retrieved similar documents, which you can further process or analyze as needed.
"""

from langchain.vectorstores import Pinecone
index_name = "finance"

# if you already have an index, you can load it like this
docsearch = Pinecone.from_existing_index(index_name, embeddings)

query = "What is a mortgage"
docs = docsearch.similarity_search(query)

print(docs[0].page_content)

print(docs[1].page_content)

"""**The provided code demonstrates how to use the `langchain.chains.RetrievalQA` class in conjunction with an OpenAI language model and a Pinecone index for retrieval-based question answering. Here's a summary explanation of the code:**

1. The `langchain.chains.RetrievalQA` class is imported, assuming it is part of the "langchain" library.

2. The `langchain.OpenAI` class is imported, assuming it is part of the "langchain" library. This class represents an OpenAI language model.

3. An instance of the `OpenAI` class is created by calling `OpenAI(temperature=0.2)`. The `temperature` parameter sets the temperature value for generating responses from the language model.

4. An instance of the `RetrievalQA` class is created by calling `RetrievalQA.from_chain_type(...)`. This initializes the `qa` object for retrieval-based question answering.

5. The `llm` parameter of `RetrievalQA.from_chain_type()` is set to the previously created `llm` OpenAI language model instance.

6. The `chain_type` parameter is set to "stuff" to specify the type of question answering chain.

7. The `retriever` parameter is set to `docsearch.as_retriever(search_kwargs={"k": 2})`. This indicates that the Pinecone index loaded in `docsearch` should be used as the retriever for question answering, and the search is performed with a `k` value of 2, which specifies that 2 most similar documents should be retrieved for each question.
"""

from langchain.chains import RetrievalQA
from langchain import OpenAI

#defining LLM
llm = OpenAI(temperature=0.2)

qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=docsearch.as_retriever(search_kwargs={"k": 2}))

embed = OpenAIEmbeddings(
    model="text-embedding-ada-002",
    openai_api_key= "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"
)

"""**The provided code initializes a `langchain.vectorstores.Pinecone` object for working with a Pinecone index in the context of the "langchain" library. Here's a summary explanation of the code:**

1. The `langchain.vectorstores.Pinecone` class is imported.

2. The variable `text_field` is set to the name of the field in the index that contains the text data. In this case, it is set to "text".

3. The `pinecone.Index('finance')` line instantiates a `pinecone.Index` object for the 'finance' index. This assumes that you have previously connected to the Pinecone service and have a valid 'finance' index.

4. The `embed.embed_query` is provided as the embedding function. 

5. The `Pinecone` object is created by passing the Pinecone index, embedding function, and text field name to the `Pinecone` class. The resulting `vectorstore` object can be used for various operations, such as searching and retrieving similar vectors based on text queries.

Please note that the `langchain` library and its specific implementation or requirements may vary. Ensure that you have the appropriate library version installed and configured, and that the necessary dependencies and functions (such as the embedding function) are available to execute this code successfully.
"""

from langchain.vectorstores import Pinecone

text_field = "text"

# switch back to normal index for langchain
index = pinecone.Index('finance')

vectorstore = Pinecone(
    index, embed.embed_query, text_field
)

"""**The provided code performs a similarity search using the `vectorstore` object, which represents a Pinecone index, to find the most relevant documents related to a given query. Here's a summary explanation of the code:**

1. The variable `query` is set to the search query, which in this case is "What is the process of taking Mortgage Loan?".

2. The `vectorstore.similarity_search()` method is called on the `vectorstore` object to perform the similarity search.

3. The search is executed with the specified parameters:
   - The `query` parameter is set to the search query.
   - The `k` parameter is set to 3, indicating that the top 3 most relevant documents should be returned.

4. The result of the similarity search is returned, providing the most relevant documents based on the given query.

Please ensure that the `vectorstore` object is properly initialized and connected to a valid Pinecone index. Additionally, make sure that the necessary dependencies and functions are available to execute this code successfully.
"""

query = "What is the process of taking Mortgage Loan?"

vectorstore.similarity_search(
    query,  # our search query
    k=3  # return 3 most relevant docs
)

"""## **Setting Up Agent**

**The provided code sets up a chat-based conversation system using the "langchain" library. Here's a summary explanation of the code:**

1. The `langchain.chat_models.ChatOpenAI` class is imported for chat-based language model completion.

2. The `langchain.chains.conversation.memory.ConversationBufferWindowMemory` class is imported to set up conversational memory.

3. The `langchain.chains.RetrievalQA` class is imported for retrieval-based question answering.

4. An instance of the `ChatOpenAI` class is created with the necessary parameters:
   - `openai_api_key` is set to your OpenAI API key.
   - `model_name` is set to 'gpt-3.5-turbo', indicating the desired OpenAI language model.
   - `temperature` is set to 0.0, which produces deterministic responses.

5. An instance of the `ConversationBufferWindowMemory` class is created for conversational memory:
   - `memory_key` is set to 'chat_history' to identify the memory key for the conversational memory.
   - `k` is set to 5, specifying the number of messages to be stored in the conversational memory.
   - `return_messages` is set to `True` to indicate that the messages in the conversational memory should be returned.

6. An instance of the `RetrievalQA` class is created for retrieval-based question answering:
   - The `llm` parameter is set to the previously created `llm` instance of `ChatOpenAI`.
   - The `chain_type` parameter is set to "stuff" to specify the type of question answering chain.
   - The `retriever` parameter is set to `vectorstore.as_retriever()` to use the `vectorstore` object as the retriever.
"""

from langchain.chat_models import ChatOpenAI
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain.chains import RetrievalQA

# chat completion llm
llm = ChatOpenAI(
    openai_api_key= "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"
,
    model_name='gpt-3.5-turbo',
    temperature=0.0
)
# conversational memory
conversational_memory = ConversationBufferWindowMemory(
    memory_key='chat_history',
    k=5,
    return_messages=True
)
# retrieval qa chain
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

qa.run(query)

"""## **Setting up Tools**

**The provided code sets up an agent using the "langchain" library, which combines tools and a chat-based language model to create conversational capabilities. Here's a summary explanation of the code:**

1. The `langchain.agents.AgentType` class is imported to specify the type of agent.

2. The `langchain.agents.initialize_agent` function is imported to initialize the agent.

3. The `langchain.memory.ConversationBufferMemory` class is imported to set up conversation memory.

4. The `langchain.chat_models.ChatOpenAI` class is imported for chat-based language model completion.

5. A list named `tools` is created, which represents the tools available to the agent. Each tool is represented by a `Tool` object with the following parameters:
   - `name` is set to the name of the tool.
   - `func` is set to the function that the tool performs (in this case, `qa.run`).
   - `description` provides a description of the tool.

6. An instance of `ConversationBufferMemory` is created to set up conversation memory, with `memory_key` set to "chat_history".

7. An instance of `ChatOpenAI` is assumed to be created earlier as `llm`.

8. The `initialize_agent` function is called to set up the agent:
   - The `tools` list is passed as the first argument.
   - The `llm` instance of `ChatOpenAI` is passed as the second argument.
   - `agent` is set to `AgentType.CONVERSATIONAL_REACT_DESCRIPTION` to specify the type of agent.
   - `verbose` is set to `True` to enable verbose logging.
   - `memory` is set to the previously created `ConversationBufferMemory` instance.
"""

from langchain.agents import AgentType
from langchain.agents import initialize_agent, Tool
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI

#defining the tools for the agent
tools = [
    Tool(
        name = "Prefit for AI",
        func=qa.run,
        description="You are a AI conversational bot made for providing the users of Homechat AI with information relating to the bank & financial aspect of the danish real estate market."
    ),
]

#setting a memory for conversations
memory = ConversationBufferMemory(memory_key="chat_history")

#Setting up the agent 
agent_chain = initialize_agent(tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)

agent_chain.run(input="What is process of taking Mortgage Loan?")

from langchain.agents import AgentType
from langchain.agents import initialize_agent, Tool
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI

#defining the tools for the agent
tools = [
    Tool(
        name = "Intermediate Answer",
        func=qa.run,
        description="useful for when you need to ask with search"
    ),
]

#setting a memory for conversations
memory = ConversationBufferMemory(memory_key="chat_history")

#Setting up the agent 
agent_chain = initialize_agent(tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)

agent_chain.run(input="What is process of taking Mortgage Loan?")

from langchain.agents import AgentType
from langchain.agents import initialize_agent, Tool
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI

#defining the tools for the agent
tools = [
    Tool(
        name = "Language Understanding",
        func=qa.run,
        description="Processes and understands natural language inputs to generate contextually appropriate responses or actions."
    ),
]

#setting a memory for conversations
memory = ConversationBufferMemory(memory_key="chat_history")

#Setting up the agent 
agent_chain = initialize_agent(tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)

agent_chain.run(input="What is process of taking Mortgage Loan?")

from langchain.agents import AgentType
from langchain.agents import initialize_agent, Tool
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI

#defining the tools for the agent
tools = [
    Tool(
        name = "Text Summarization",
        func=qa.run,
        description="Summarizes long documents or pieces of text into shorter, concise summaries."
    ),
]

#setting a memory for conversations
memory = ConversationBufferMemory(memory_key="chat_history")

#Setting up the agent 
agent_chain = initialize_agent(tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)

agent_chain.run(input="What is process of taking Mortgage Loan?")

"""## **Agent Tookit**

### **SQL Agent Toolkit**

**This command will install the `python-dotenv` package, which is a Python library used for parsing `.env` files and loading environment variables into your Python script or application. After running the installation command, you should be able to import and use the `dotenv` module in your code.**
"""

!pip install python-dotenv

"""**The provided code snippet imports the necessary modules and sets up environment variables for working with the "langchain" library, OpenAI, and SQL databases. Here's a summary explanation of the code:**

1. The `langchain.OpenAI`, `langchain.SQLDatabase`, and `langchain.SQLDatabaseChain` classes are imported from the "langchain" library.

2. The `dotenv.load_dotenv()` function is imported from the `dotenv` module. This function is used to load environment variables from a `.env` file.

3. The `os` module is imported to work with operating system-related functionality.

4. The `load_dotenv()` function is called to load environment variables from a `.env` file in the current directory. This assumes that you have a `.env` file with the necessary environment variable declarations.

5. The `os.environ.get()` function is used to retrieve the value of the environment variable named `API key` and assign it to the variable `OPENAI_API_KEY`. Note that the `os.environ.get()` function is used with the exact name of the environment variable. If the name of the environment variable is actually `API_key`, you need to ensure that it matches the environment variable name.

6. The `os.environ["OPENAI_API_KEY"] = "API key"` line sets the environment variable `OPENAI_API_KEY` directly in the `os.environ` dictionary. This assigns the value "API key" to the environment variable.
"""

from langchain import OpenAI, SQLDatabase, SQLDatabaseChain
from dotenv import load_dotenv
import os

load_dotenv()
OPENAI_API_KEY = os.environ.get('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')

os.environ["OPENAI_API_KEY"] = "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"

"""**This command uses the `!pip` command in a Jupyter Notebook or IPython environment to install the `langchain`, `openai`, and `pymysql` packages. The `--upgrade` flag ensures that the packages are upgraded to their latest versions, and the `-q` flag suppresses unnecessary output during installation.**"""

!pip install  langchain openai pymysql --upgrade -q

"""**The provided code imports various modules and classes from the "langchain" library to set up an agent with SQL database integration. Here's a summary explanation of the code:**

1. The `langchain.agents.load_tools` function is imported to load agent tools.

2. The `langchain.agents.initialize_agent` function is imported to initialize the agent.

3. The `langchain.agents.AgentType` class is imported to specify the type of agent.

4. The `os` module is imported to work with operating system-related functionality.

5. The `langchain.agents.create_sql_agent` function is imported to create an agent with SQL database integration.

6. The `langchain.agents.agent_toolkits.SQLDatabaseToolkit` class is imported to handle SQL database operations.

7. The `langchain.sql_database.SQLDatabase` class is imported to work with SQL databases.

8. The `langchain.llms.openai.OpenAI` class is imported for OpenAI language model integration.

9. The `langchain.agents.AgentExecutor` class is imported to execute agent tasks.
"""

from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType
import os
from langchain.agents import create_sql_agent
from langchain.agents.agent_toolkits import SQLDatabaseToolkit
from langchain.sql_database import SQLDatabase
from langchain.llms.openai import OpenAI
from langchain.agents import AgentExecutor

"""**Setting up the MySQL Database with remote server**"""

db_user = "sql7621852"
db_password = "Ftcxu2kyLJ"
db_host = "sql7.freesqldatabase.com"
db_name = "sql7621852"
db = SQLDatabase.from_uri(f"mysql+pymysql://sql7621852:Ftcxu2kyLJ@sql7.freesqldatabase.com:3306/sql7621852")

"""**The provided code initializes an instance of the `OpenAI` class from the "langchain" library for OpenAI language model integration. Here's a summary explanation of the code:**

1. The `langchain.llms.openai.OpenAI` class is imported.

2. An instance of the `OpenAI` class is created with the following parameter:
   - `temperature` is set to 0, which indicates a deterministic mode where the language model produces consistent responses.

3. The resulting instance is assigned to the variable `llm`, representing the OpenAI language model.
"""

llm = OpenAI(temperature=0)

import pinecone

# Init Pinecone with your API key
pinecone.init(api_key="47cb1de2-5bd4-4166-b4e8-bb7ae9db8393")

# Get a reference to your Pinecone Index
vectorstore = pinecone.Index(index_name="finance")

"""**The provided code initializes an instance of the `SQLDatabaseToolkit` class from the "langchain" library, which integrates SQL database operations with a vector store and an OpenAI language model. Here's a summary explanation of the code:**

1. The `langchain.agents.agent_toolkits.SQLDatabaseToolkit` class is imported.

2. An instance of the `SQLDatabaseToolkit` class is created with the following parameters:
   - `db` represents the SQL database object.
   - `vectorstore` represents the vector store object.
   - `llm` represents the OpenAI language model object.
   - `verbose` is set to `True` to enable verbose logging.

3. The resulting instance is assigned to the variable `toolkit`, which can be used for SQL database operations within the agent.
"""

toolkit = SQLDatabaseToolkit(db=db, vectorstore=vectorstore, llm=llm, verbose=True)

"""**The provided code creates an agent executor with SQL database integration using the "langchain" library. Here's a summary explanation of the code:**

1. The `langchain.agents.create_sql_agent` function is imported.

2. The `create_sql_agent` function is called with the following parameters:
   - `llm` represents the OpenAI language model object.
   - `toolkit` represents the SQL database toolkit object.

3. The resulting agent executor is assigned to the variable `agent_executor`.

The `agent_executor` is now ready to execute agent tasks that involve SQL database operations, leveraging the integration provided by the SQL database toolkit.
"""

agent_executor = create_sql_agent(
    llm=llm,
    toolkit=toolkit,
)

agent_executor.run("What is Mortgage Loan?")

toolkit = SQLDatabaseToolkit(db=db, llm=llm)

agent_executor = create_sql_agent(
    llm=OpenAI(temperature=0),
    toolkit=toolkit,
    index='finance',
    verbose=True
)

"""#### **Agent Executor**

**In the "langchain" library, the `AgentExecutor` class provides the execution framework for running agents and handling their interactions. It coordinates the flow of information between the agent's tools, language models, and any other components involved in the agent's logic. Here's a general overview of how the `AgentExecutor` works in the context of a language chain agent:**

1. Initialization: The `AgentExecutor` is typically instantiated by providing the necessary components and configurations for the agent. This includes the language model (e.g., an instance of `OpenAI`), agent tools, memory, or any other required objects.

2. Input Handling: The `AgentExecutor` receives an input query or message that needs to be processed by the agent. This input can be a user query or a message from a conversation.

3. Tool Selection: Based on the input and agent configuration, the `AgentExecutor` determines which tool or tools should handle the input. This decision can be based on various factors, such as the type of input, the current agent state, or any other relevant criteria.

4. Tool Execution: The selected tool or tools are executed to process the input. This can involve performing specific actions, making queries to databases, generating responses using language models, or any other operations defined by the tool's functionality.

5. Response Generation: After the tool execution, the `AgentExecutor` collects the outputs from the tools and combines them, if necessary, to generate a response. This response can be in the form of text, structured data, or any other format defined by the agent's logic.

6. Memory Update: If the agent has a memory component, the `AgentExecutor` updates the memory with the relevant information from the input and the tool execution. This ensures that the agent's context is preserved and can influence future interactions.

7. Output Delivery: Finally, the generated response is returned or sent back to the user or conversation context, completing the interaction.

The specific implementation and behavior of the `AgentExecutor` can vary depending on the agent's design, the specific tools used, and the interactions with external components such as databases or language models.
"""

agent_executor.run("What is Mortgage Loan?")

"""### **Python Agent Toolkit**

**The provided code imports modules and classes from the "langchain" library to set up a Python agent with a Python REPL tool and integration with an OpenAI language model. Here's a summary explanation of the code:**

1. The `langchain.agents.agent_toolkits.create_python_agent` function is imported.

2. The `langchain.tools.python.tool.PythonREPLTool` class is imported for the Python REPL tool.

3. The `langchain.python.PythonREPL` class is imported for the Python REPL integration.

4. The `langchain.llms.openai.OpenAI` class is imported for OpenAI language model integration.

5. The `create_python_agent` function is called to create a Python agent:
   - The `PythonREPLTool` instance is passed as the first argument to represent the Python REPL tool.
   - An instance of `PythonREPL` is created internally by the `create_python_agent` function.
   - An instance of `OpenAI` is passed as the second argument to represent the OpenAI language model.

The resulting Python agent can execute Python code interactively using the Python REPL tool and leverage the OpenAI language model for generating responses.
"""

from langchain.agents.agent_toolkits import create_python_agent
from langchain.tools.python.tool import PythonREPLTool
from langchain.python import PythonREPL
from langchain.llms.openai import OpenAI

"""**The provided code initializes an agent executor with a Python agent and a Python REPL tool using the "langchain" library. Here's a summary explanation of the code:**

1. The `create_python_agent` function is imported from `langchain.agents.agent_toolkits`.

2. The `OpenAI` class is imported from `langchain.llms.openai` for OpenAI language model integration.

3. The `PythonREPLTool` class is imported from `langchain.tools.python.tool` for the Python REPL tool.

4. An instance of the `OpenAI` class is created with the following parameters:
   - `temperature` is set to 0, which indicates a deterministic mode where the language model produces consistent responses.
   - `max_tokens` is set to 1000, which limits the response length to 1000 tokens.

5. An instance of the `PythonREPLTool` class is created to represent the Python REPL tool.

6. The `create_python_agent` function is called to create an agent executor with a Python agent:
   - The `llm` parameter is set to the previously created `OpenAI` instance.
   - The `tool` parameter is set to the `PythonREPLTool` instance.
   - `verbose` is set to `True` to enable verbose logging.

7. The resulting agent executor is assigned to the variable `agent_executor`.

The `agent_executor` is now ready to execute Python code interactively using the Python REPL tool and generate responses with the OpenAI language model.
"""

agent_executor = create_python_agent(
    llm=OpenAI(temperature=0, max_tokens=1000),
    tool=PythonREPLTool(),
    verbose=True
)

"""#### **Agent Executor**

**In the "langchain" library, the `AgentExecutor` class provides the execution framework for running agents and handling their interactions. It coordinates the flow of information between the agent's tools, language models, and any other components involved in the agent's logic. Here's a general overview of how the `AgentExecutor` works in the context of a language chain agent:**

1. Initialization: The `AgentExecutor` is typically instantiated by providing the necessary components and configurations for the agent. This includes the language model (e.g., an instance of `OpenAI`), agent tools, memory, or any other required objects.

2. Input Handling: The `AgentExecutor` receives an input query or message that needs to be processed by the agent. This input can be a user query or a message from a conversation.

3. Tool Selection: Based on the input and agent configuration, the `AgentExecutor` determines which tool or tools should handle the input. This decision can be based on various factors, such as the type of input, the current agent state, or any other relevant criteria.

4. Tool Execution: The selected tool or tools are executed to process the input. This can involve performing specific actions, making queries to databases, generating responses using language models, or any other operations defined by the tool's functionality.

5. Response Generation: After the tool execution, the `AgentExecutor` collects the outputs from the tools and combines them, if necessary, to generate a response. This response can be in the form of text, structured data, or any other format defined by the agent's logic.

6. Memory Update: If the agent has a memory component, the `AgentExecutor` updates the memory with the relevant information from the input and the tool execution. This ensures that the agent's context is preserved and can influence future interactions.

7. Output Delivery: Finally, the generated response is returned or sent back to the user or conversation context, completing the interaction.

The specific implementation and behavior of the `AgentExecutor` can vary depending on the agent's design, the specific tools used, and the interactions with external components such as databases or language models.
"""

agent_executor.run("What is the process of taking Mortgage Loan?")